# Spark Examples in Python (pyspark)

## **Spark Core**

1.	**SparkContext**

	- 	Parallelized Collections
	- 	External Datasets
		- 	textFile
		- 	wholeTextFile
		-	sequenceFile
	-	addFile 

2.	**Resilient Distributed Datasets (RDDs)**

	-	Passing function to RDD
	-	aggregate
	-	aggregateByKey
	-	cogroup
	-	combineByKey
	-	count, countByKey, countByValue
	-	filter
	-	fold, foldByKey
	-	foreach, foreachPartition
	-	fullOuterJoin
	-	groupBy, groupByKey
	-	join
	-	leftOuterJoin
	-	map, flatMap, mapPartitions
	-	persist
	-	reduce, reduceByKey
	-	repartition, repartitionAndSortWithinPartitions
	-	rightOuterJoin
	-	Set Operations - cartesian, intersection, union
	-	zip
	
	
## 	**Spark SQL**

	- 	DF - JSON
	-	Converting existing RDDs into Datasets
		-	Inferring the schema
		-	Specifying the schema
	-	Load and Save
	-	Bucket, Sort and Partition
	-	DF - Parquet
	-	Register UDF
	-	explain
	-	fillna
	-	select, filter, groupBy
	-	createOrReplaceTempView
	-	createGlobalTempView
	

## **Structured Streaming**




