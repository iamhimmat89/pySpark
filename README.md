# Spark Examples in Python (pyspark)

## **Spark Core**
1.	**SparkContext**
	- 	Parallelized Collections
	- 	External Datasets
		- 	textFile
		- 	wholeTextFile
		-	sequenceFile
	-	addFile 

2.	**Resilient Distributed Datasets (RDDs)**
	-	Passing function to RDD
	-	aggregate
	-	aggregateByKey
	-	cogroup
	-	combineByKey
	-	count, countByKey, countByValue
	-	filter
	-	fold, foldByKey
	-	foreach, foreachPartition
	-	fullOuterJoin
	-	groupBy, groupByKey
	-	join
	-	leftOuterJoin
	-	map, flatMap, mapPartitions
	-	persist
	-	reduce, reduceByKey
	-	repartition, repartitionAndSortWithinPartitions
	-	rightOuterJoin
	-	Set Operations - cartesian, intersection, union
	-	zip
	
		
## **Spark SQL**



## **Spark Streaming**




